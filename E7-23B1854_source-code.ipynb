{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9802143,"sourceType":"datasetVersion","datasetId":6007686},{"sourceId":9819523,"sourceType":"datasetVersion","datasetId":6000757}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Team Billie Jean\n**DS203 Project, Autumn 2024**\n\nTeam Members:\n1. Chinmay Kale, 23B1849 **[Anchor]**\n2. Gokularamanan R S, 23B1854\n3. Arya Sameer Joshi, 23B1853\n4. Arash Dev Ahlawat, 23B1817","metadata":{"_uuid":"9517110b-718c-4a41-a179-6d5d8f03f415","_cell_guid":"e88d4ac8-7dee-44d4-a87c-c14206434510","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport tabulate\nimport glob\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport librosa\n\n# Configuration\nNUM_CLASSES = 6  # Updated number of classes based on given labels\nBATCH_SIZE = 20\nEPOCHS = 40\nLEARNING_RATE = 0.005\nGRAD_CLIP = 5\nDATA_FOLDER = r\"/kaggle/input/train-songs/Train_data_songs/All_CSV_Files\"\n\n# Define labels based on keywords in filenames\nLABEL_MAP = {\n    \"asha bhosale\": 0,\n    \"jana gana mana\": 1,\n    \"kishore kumar\": 2,\n    \"marathi bhavgeet\": 3,\n    \"marathi lavni\": 4,\n    \"michael jackson\": 5\n}\n\n# Specify the length of each chunk\nCHUNK_LENGTH = 500\n\ndef load_data_from_csv(folder_path):\n    mfcc_data = []\n    labels = {0:[],1:[],2:[],3:[],4:[],5:[]}\n\n    # Iterate through all CSV files in the folder\n    for file_path in glob.glob(os.path.join(folder_path, '*.csv')):\n        # Load MFCC data from CSV\n        mfcc = np.loadtxt(file_path, delimiter=',')\n        \n        # Calculate delta MFCC\n        delta_mfcc = librosa.feature.delta(mfcc)\n\n        # Concatenate MFCC and delta MFCC along the first axis (number of coefficients)\n        mfcc_with_delta = np.concatenate((mfcc, delta_mfcc), axis=0)\n\n        # Determine label based on filename\n        file_name = os.path.basename(file_path).lower()\n        label = -1  # Default label if no keyword matches\n        for keyword, lbl in LABEL_MAP.items():\n            if keyword in file_name:\n                label = lbl\n\n                break\n        \n        # If no keyword matches, skip this file\n        if label == -1:\n            continue\n\n        # Calculate total columns and divide into chunks of specified length\n        total_columns = mfcc_with_delta.shape[1]  # Get the number of columns (time series)\n        timme = total_columns/(60*86)\n        labels[lbl].append(timme)\n        # Loop to divide data into chunks of CHUNK_LENGTH\n        for start_idx in range(0, total_columns, CHUNK_LENGTH):\n            end_idx = start_idx + CHUNK_LENGTH\n            # Ignore the last chunk if it's less than CHUNK_LENGTH\n            if end_idx > total_columns:\n                break\n            \n            # Create a chunk and add to the data\n            chunk = mfcc_with_delta[:, start_idx:end_idx]  # Slice along columns\n            mfcc_data.append(chunk)\n\n    return mfcc_data, labels\n\n\n# Load data and split into train/test sets\nmfcc_data, labels = load_data_from_csv(DATA_FOLDER)\nfor i in labels:\n    labels[i] = np.mean(labels[i])\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(mfcc_data, labels_encoded, test_size=0.3, random_state=341)","metadata":{"_uuid":"c253672b-0650-4935-906c-90014b8c07e9","_cell_guid":"6983256f-caa2-4ab8-b576-df19e15f2626","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-09T18:29:06.238712Z","iopub.execute_input":"2024-11-09T18:29:06.239210Z","iopub.status.idle":"2024-11-09T18:29:19.950661Z","shell.execute_reply.started":"2024-11-09T18:29:06.239169Z","shell.execute_reply":"2024-11-09T18:29:19.949874Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"for i in labels:\n    labels[i] = np.mean(labels[i])\nprint(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T18:30:03.372752Z","iopub.execute_input":"2024-11-09T18:30:03.373609Z","iopub.status.idle":"2024-11-09T18:30:03.378959Z","shell.execute_reply.started":"2024-11-09T18:30:03.373559Z","shell.execute_reply":"2024-11-09T18:30:03.377943Z"}},"outputs":[{"name":"stdout","text":"{0: 4.725175043760941, 1: 1.5278985507246376, 2: 4.710581395348838, 3: 4.308885658914728, 4: 4.540010002500624, 5: 4.604638659664917}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Define custom dataset\nclass MFCCDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        mfcc = torch.tensor(self.data[idx], dtype=torch.float32).squeeze()  # (128, time)\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        return mfcc, label, mfcc.shape[1]  # Include length of each sequence\n\n# Collate function to handle variable-length sequences\n# Collate function to handle variable-length sequences\n# Collate function to handle variable-length sequences\n\n# def collate_fn(batch):\n#     data, labels, lengths = zip(*batch)\n    \n#     # Find the maximum length in the batch\n#     max_length = max([d.shape[1] for d in data])\n\n#     # Pad each tensor to the maximum length and stack them\n#     data_padded = [torch.nn.functional.pad(d, (0, 0, 0, max_length - d.shape[1])) for d in data]\n    \n#     # Stack padded data\n#     data_padded = torch.stack(data_padded)\n\n#     # Convert lengths and labels to tensors\n#     lengths = torch.tensor([min(d.shape[1], max_length) for d in data], dtype=torch.int64)\n#     labels = torch.tensor(labels, dtype=torch.long)\n\n#     return data_padded, labels, lengths\n\ndef collate_fn(batch):\n    data, labels, lengths = zip(*batch)\n    \n    # Stack data and labels directly, as all sequences are the same length\n    data_stacked = torch.stack(data)\n    labels = torch.tensor(labels, dtype=torch.long)\n    lengths = torch.tensor(lengths, dtype=torch.int64)  # Optional if lengths are all the same\n\n    return data_stacked, labels, lengths\n\n\n# Data loaders\ntrain_dataset = MFCCDataset(X_train, y_train)\ntest_dataset = MFCCDataset(X_test, y_test)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)","metadata":{"_uuid":"c4e2a94f-9a94-4458-ad39-ac14d19d98ba","_cell_guid":"a394013f-fafc-42ec-9ab7-e202dbfae1ad","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-09T17:41:04.316627Z","iopub.execute_input":"2024-11-09T17:41:04.317507Z","iopub.status.idle":"2024-11-09T17:41:04.327666Z","shell.execute_reply.started":"2024-11-09T17:41:04.317458Z","shell.execute_reply":"2024-11-09T17:41:04.326785Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"print(labels_encoded)\n#print(mfcc_data)","metadata":{"_uuid":"101f7f52-47b1-4710-b6dc-95409cef5b68","_cell_guid":"b46c50ac-d897-4295-a589-1da22f23624a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-09T17:41:04.329135Z","iopub.execute_input":"2024-11-09T17:41:04.329458Z","iopub.status.idle":"2024-11-09T17:41:04.345929Z","shell.execute_reply.started":"2024-11-09T17:41:04.329411Z","shell.execute_reply":"2024-11-09T17:41:04.344800Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"[5 5 5 ... 4 4 4]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class CNNRNNModel(nn.Module):\n    def __init__(self, num_classes):\n        super(CNNRNNModel, self).__init__()\n        \n        # Convolutional layers with batch normalization\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)  # Make sure this is defined\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)  # Make sure this is defined\n        \n        # LSTM layer\n        self.lstm = nn.LSTM(640, 128, batch_first=True)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(128, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n        self.dropout = nn.Dropout(0.01)\n\n    def forward(self, x, lengths):\n        x = x.unsqueeze(1)  # Add channel dimension\n        \n        # Pass through convolutional and batch normalization layers\n        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n        \n        # Reshape for LSTM input\n        batch_size, _, conv_height, conv_width = x.shape\n        x = x.permute(0, 3, 1, 2).reshape(batch_size, conv_width, conv_height * 64)\n        \n        # Pack sequences and pass through LSTM\n        max_seq_length = x.size(1)\n        lengths = torch.clamp(lengths, max=max_seq_length)\n        x = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        x, _ = self.lstm(x)\n        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n        \n        # Global average pooling and fully connected layers\n        x = torch.mean(x, dim=1)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x","metadata":{"_uuid":"4543276c-327c-4e25-b5f3-48c2f06c9a24","_cell_guid":"2aaa4f35-e2f2-4fae-937a-04d52c00d507","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-09T17:41:04.347286Z","iopub.execute_input":"2024-11-09T17:41:04.348113Z","iopub.status.idle":"2024-11-09T17:41:04.361184Z","shell.execute_reply.started":"2024-11-09T17:41:04.348067Z","shell.execute_reply":"2024-11-09T17:41:04.360300Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Initialize model, loss, and optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Running on GPU\" if torch.cuda.is_available() else \"Running on CPU\")\nmodel = CNNRNNModel(num_classes=NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n# Training function\ndef train(model, train_loader, criterion, optimizer):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels, lengths in train_loader:\n        # Move data to GPU\n        inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs, lengths)\n        loss = criterion(outputs, labels)\n        loss.backward()\n\n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n\n        optimizer.step()\n        running_loss += loss.item()\n    return running_loss / len(train_loader)\n\ndef evaluate(model, test_loader):\n    model.eval()\n    correct, total = 0, 0\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for inputs, labels, lengths in test_loader:\n            # Move data to GPU\n            inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n\n            outputs = model(inputs, lengths)\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n            y_true.extend(labels.cpu().tolist())\n            y_pred.extend(predicted.cpu().tolist())\n\n    acc = accuracy_score(y_true, y_pred)\n    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    rec = recall_score(y_true, y_pred, average='weighted')\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    return acc, prec, rec, f1\n\nprint(\"Vinmay\")","metadata":{"_uuid":"88ca240d-e330-47fa-8da7-e12ebcc6999e","_cell_guid":"740907d1-3ba5-46fd-9a7a-369266cd9c8d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-09T17:41:04.363189Z","iopub.execute_input":"2024-11-09T17:41:04.363523Z","iopub.status.idle":"2024-11-09T17:41:05.706792Z","shell.execute_reply.started":"2024-11-09T17:41:04.363485Z","shell.execute_reply":"2024-11-09T17:41:05.705746Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Running on GPU\nVinmay\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Initialize best_f1 before the training loop\nbest_acc = 0.0  # or float('-inf') if you want to allow for any positive f1 score initially\npatience = 5\nepochs_without_improvement = 0\n\nfor epoch in range(EPOCHS):\n    train_loss = train(model, train_loader, criterion, optimizer)\n    acc, prec, rec, f1 = evaluate(model, test_loader)\n    print(f'Epoch {epoch + 1}/{EPOCHS}, Loss: {train_loss:.4f}, Acc: {acc:.8f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}')\n    \n    # Update the learning rate scheduler\n    scheduler.step()\n\n    # Save the best model\n    \n    if acc > best_acc:\n        best_acc = acc\n        epochs_without_improvement = 0\n        torch.save(model.state_dict(), 'best_model.pth')\n        print(f\"Best model saved with Accuracy: {best_acc:.5f}\")\n    else:\n        epochs_without_improvement += 1\ntorch.save(model.state_dict(), 'final_model_state.pth')\nprint(\"Training complete.\")\nprint(\"Final model saved.\")","metadata":{"_uuid":"1df09916-334b-45c6-b376-d25a56285d89","_cell_guid":"a3dd8b38-e4a9-4f53-b64f-c1b1123dc96f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-09T17:41:05.708046Z","iopub.execute_input":"2024-11-09T17:41:05.708495Z","iopub.status.idle":"2024-11-09T17:45:12.285993Z","shell.execute_reply.started":"2024-11-09T17:41:05.708461Z","shell.execute_reply":"2024-11-09T17:45:12.284956Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Epoch 1/40, Loss: 1.0820, Acc: 0.65649166, Prec: 0.7018, Rec: 0.6565, F1: 0.6516\nBest model saved with Accuracy: 0.65649\nEpoch 2/40, Loss: 0.8306, Acc: 0.73748474, Prec: 0.7373, Rec: 0.7375, F1: 0.7294\nBest model saved with Accuracy: 0.73748\nEpoch 3/40, Loss: 0.7002, Acc: 0.74603175, Prec: 0.7563, Rec: 0.7460, F1: 0.7437\nBest model saved with Accuracy: 0.74603\nEpoch 4/40, Loss: 0.6085, Acc: 0.78021978, Prec: 0.7917, Rec: 0.7802, F1: 0.7785\nBest model saved with Accuracy: 0.78022\nEpoch 5/40, Loss: 0.5196, Acc: 0.81400081, Prec: 0.8233, Rec: 0.8140, F1: 0.8148\nBest model saved with Accuracy: 0.81400\nEpoch 6/40, Loss: 0.3648, Acc: 0.84940985, Prec: 0.8515, Rec: 0.8494, F1: 0.8496\nBest model saved with Accuracy: 0.84941\nEpoch 7/40, Loss: 0.3210, Acc: 0.85958486, Prec: 0.8608, Rec: 0.8596, F1: 0.8597\nBest model saved with Accuracy: 0.85958\nEpoch 8/40, Loss: 0.2960, Acc: 0.86894587, Prec: 0.8710, Rec: 0.8689, F1: 0.8688\nBest model saved with Accuracy: 0.86895\nEpoch 9/40, Loss: 0.2728, Acc: 0.87830688, Prec: 0.8787, Rec: 0.8783, F1: 0.8778\nBest model saved with Accuracy: 0.87831\nEpoch 10/40, Loss: 0.2524, Acc: 0.88074888, Prec: 0.8816, Rec: 0.8807, F1: 0.8805\nBest model saved with Accuracy: 0.88075\nEpoch 11/40, Loss: 0.2265, Acc: 0.89214489, Prec: 0.8916, Rec: 0.8921, F1: 0.8912\nBest model saved with Accuracy: 0.89214\nEpoch 12/40, Loss: 0.2208, Acc: 0.89214489, Prec: 0.8916, Rec: 0.8921, F1: 0.8913\nEpoch 13/40, Loss: 0.2214, Acc: 0.89214489, Prec: 0.8926, Rec: 0.8921, F1: 0.8919\nEpoch 14/40, Loss: 0.2139, Acc: 0.89377289, Prec: 0.8935, Rec: 0.8938, F1: 0.8932\nBest model saved with Accuracy: 0.89377\nEpoch 15/40, Loss: 0.2138, Acc: 0.89621490, Prec: 0.8963, Rec: 0.8962, F1: 0.8957\nBest model saved with Accuracy: 0.89621\nEpoch 16/40, Loss: 0.2101, Acc: 0.89540090, Prec: 0.8953, Rec: 0.8954, F1: 0.8950\nEpoch 17/40, Loss: 0.2111, Acc: 0.89540090, Prec: 0.8949, Rec: 0.8954, F1: 0.8947\nEpoch 18/40, Loss: 0.2108, Acc: 0.89621490, Prec: 0.8958, Rec: 0.8962, F1: 0.8957\nEpoch 19/40, Loss: 0.2112, Acc: 0.89499389, Prec: 0.8945, Rec: 0.8950, F1: 0.8944\nEpoch 20/40, Loss: 0.2114, Acc: 0.89499389, Prec: 0.8945, Rec: 0.8950, F1: 0.8945\nEpoch 21/40, Loss: 0.2080, Acc: 0.89458689, Prec: 0.8942, Rec: 0.8946, F1: 0.8940\nEpoch 22/40, Loss: 0.2101, Acc: 0.89417989, Prec: 0.8936, Rec: 0.8942, F1: 0.8936\nEpoch 23/40, Loss: 0.2092, Acc: 0.89499389, Prec: 0.8945, Rec: 0.8950, F1: 0.8944\nEpoch 24/40, Loss: 0.2097, Acc: 0.89662190, Prec: 0.8961, Rec: 0.8966, F1: 0.8960\nBest model saved with Accuracy: 0.89662\nEpoch 25/40, Loss: 0.2096, Acc: 0.89540090, Prec: 0.8950, Rec: 0.8954, F1: 0.8948\nEpoch 26/40, Loss: 0.2080, Acc: 0.89499389, Prec: 0.8946, Rec: 0.8950, F1: 0.8945\nEpoch 27/40, Loss: 0.2098, Acc: 0.89458689, Prec: 0.8941, Rec: 0.8946, F1: 0.8940\nEpoch 28/40, Loss: 0.2092, Acc: 0.89662190, Prec: 0.8961, Rec: 0.8966, F1: 0.8961\nEpoch 29/40, Loss: 0.2086, Acc: 0.89540090, Prec: 0.8950, Rec: 0.8954, F1: 0.8949\nEpoch 30/40, Loss: 0.2087, Acc: 0.89540090, Prec: 0.8949, Rec: 0.8954, F1: 0.8948\nEpoch 31/40, Loss: 0.2087, Acc: 0.89662190, Prec: 0.8962, Rec: 0.8966, F1: 0.8961\nEpoch 32/40, Loss: 0.2094, Acc: 0.89499389, Prec: 0.8943, Rec: 0.8950, F1: 0.8943\nEpoch 33/40, Loss: 0.2090, Acc: 0.89621490, Prec: 0.8957, Rec: 0.8962, F1: 0.8956\nEpoch 34/40, Loss: 0.2076, Acc: 0.89662190, Prec: 0.8961, Rec: 0.8966, F1: 0.8960\nEpoch 35/40, Loss: 0.2066, Acc: 0.89458689, Prec: 0.8941, Rec: 0.8946, F1: 0.8941\nEpoch 36/40, Loss: 0.2093, Acc: 0.89580790, Prec: 0.8954, Rec: 0.8958, F1: 0.8953\nEpoch 37/40, Loss: 0.2060, Acc: 0.89540090, Prec: 0.8949, Rec: 0.8954, F1: 0.8948\nEpoch 38/40, Loss: 0.2115, Acc: 0.89580790, Prec: 0.8954, Rec: 0.8958, F1: 0.8952\nEpoch 39/40, Loss: 0.2084, Acc: 0.89499389, Prec: 0.8948, Rec: 0.8950, F1: 0.8945\nEpoch 40/40, Loss: 0.2100, Acc: 0.89377289, Prec: 0.8933, Rec: 0.8938, F1: 0.8932\nTraining complete.\nFinal model saved.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport torch\nimport librosa\nfrom collections import Counter\n\n# Load the trained model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CNNRNNModel(num_classes=NUM_CLASSES)  # Ensure NUM_CLASSES is defined\nmodel.load_state_dict(torch.load('best_model.pth'))\nmodel = model.to(device)\nmodel.eval()\n\n# Set the chunk length\n\nCHUNK_LENGTH = 500 \n# Function to load and split data into chunks of a specified length, with delta MFCC appended\ndef load_unknown_data(file_path):\n    # Load MFCC data from the CSV\n    mfcc = np.loadtxt(file_path, delimiter=',')\n\n    # Ensure 20 coefficients\n    if mfcc.shape[0] != 20:\n        raise ValueError(f\"Expected 20 MFCC coefficients, but got {mfcc.shape[0]}.\")\n\n    # Calculate delta MFCC\n    delta_mfcc = librosa.feature.delta(mfcc)\n\n    # Concatenate MFCC and delta MFCC along the first axis\n    mfcc_with_delta = np.concatenate((mfcc, delta_mfcc), axis=0)\n\n    # Calculate the total columns along the time dimension\n    total_columns = mfcc_with_delta.shape[1]\n    \n    # Split into chunks of specified length\n    chunks = []\n    for start_idx in range(0, total_columns, CHUNK_LENGTH):\n        end_idx = start_idx + CHUNK_LENGTH\n        \n        # Ignore the last chunk if it is shorter than CHUNK_LENGTH\n        if end_idx > total_columns:\n            break\n        \n        chunk = mfcc_with_delta[:, start_idx:end_idx]\n        chunks.append(chunk)\n\n    return chunks  # List of (40, CHUNK_LENGTH) arrays\n\n# Predict function for each file, using majority voting across chunks\ndef predict_label(file_path):\n    chunks = load_unknown_data(file_path)\n    chunk_predictions = []\n\n    for chunk in chunks:\n        # Prepare the chunk for model input\n        chunk_tensor = torch.tensor(chunk, dtype=torch.float32).unsqueeze(0).to(device)  # Shape: (1, 40, CHUNK_LENGTH)\n        length = torch.tensor([chunk_tensor.shape[2]], dtype=torch.int64).to(device)  # Use the time dimension\n\n        with torch.no_grad():\n            output = model(chunk_tensor, length)  # Pass both data and length to the model\n            _, predicted_label = torch.max(output, 1)\n            chunk_predictions.append(predicted_label.item())\n\n    # Find the most common prediction among the chunks\n    most_common_prediction = Counter(chunk_predictions).most_common(1)[0][0]\n    return most_common_prediction\n\n# Folder path containing unknown files for prediction\nUNKNOWN_FOLDER = \"/kaggle/input/mfcc-official/MFCC-files-v2\"\npredictions = {\"file\":[],\"label\":[]}\n# Iterate over each file in the unknown folder and print the majority prediction\nfor file_path in glob.glob(os.path.join(UNKNOWN_FOLDER, '*.csv')):\n    label = predict_label(file_path)\n    file_name = os.path.basename(file_path)\n    predictions[\"file\"].append(file_name)\n    predictions[\"label\"].append(label)\n    print(f\"File: {file_name} -> Predicted Label: {label}\")\n\nimport datetime as dt\nimport pandas as pd\npr = pd.DataFrame(predictions).sort_values(by=['file'])\npr.to_csv(f\"predictions_{dt.datetime.now()}.csv\", index = False)","metadata":{"_uuid":"689eff21-fcb4-4621-a048-c6a1f87d7f60","_cell_guid":"cbcbacff-af75-4307-9d04-5824df2b51a7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-09T17:45:12.287455Z","iopub.execute_input":"2024-11-09T17:45:12.287960Z","iopub.status.idle":"2024-11-09T17:45:41.735080Z","shell.execute_reply.started":"2024-11-09T17:45:12.287914Z","shell.execute_reply":"2024-11-09T17:45:41.734305Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3344972486.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"File: 70-MFCC.csv -> Predicted Label: 4\nFile: 104-MFCC.csv -> Predicted Label: 4\nFile: 67-MFCC.csv -> Predicted Label: 3\nFile: 110-MFCC.csv -> Predicted Label: 0\nFile: 92-MFCC.csv -> Predicted Label: 3\nFile: 33-MFCC.csv -> Predicted Label: 4\nFile: 86-MFCC.csv -> Predicted Label: 1\nFile: 29-MFCC.csv -> Predicted Label: 3\nFile: 65-MFCC.csv -> Predicted Label: 2\nFile: 16-MFCC.csv -> Predicted Label: 1\nFile: 63-MFCC.csv -> Predicted Label: 2\nFile: 01-MFCC.csv -> Predicted Label: 1\nFile: 44-MFCC.csv -> Predicted Label: 0\nFile: 03-MFCC.csv -> Predicted Label: 4\nFile: 111-MFCC.csv -> Predicted Label: 2\nFile: 72-MFCC.csv -> Predicted Label: 4\nFile: 48-MFCC.csv -> Predicted Label: 2\nFile: 53-MFCC.csv -> Predicted Label: 4\nFile: 25-MFCC.csv -> Predicted Label: 4\nFile: 91-MFCC.csv -> Predicted Label: 4\nFile: 10-MFCC.csv -> Predicted Label: 4\nFile: 23-MFCC.csv -> Predicted Label: 0\nFile: 94-MFCC.csv -> Predicted Label: 4\nFile: 74-MFCC.csv -> Predicted Label: 4\nFile: 64-MFCC.csv -> Predicted Label: 4\nFile: 79-MFCC.csv -> Predicted Label: 0\nFile: 46-MFCC.csv -> Predicted Label: 2\nFile: 06-MFCC.csv -> Predicted Label: 4\nFile: 83-MFCC.csv -> Predicted Label: 2\nFile: 09-MFCC.csv -> Predicted Label: 2\nFile: 73-MFCC.csv -> Predicted Label: 4\nFile: 36-MFCC.csv -> Predicted Label: 2\nFile: 32-MFCC.csv -> Predicted Label: 5\nFile: 61-MFCC.csv -> Predicted Label: 5\nFile: 112-MFCC.csv -> Predicted Label: 4\nFile: 107-MFCC.csv -> Predicted Label: 1\nFile: 77-MFCC.csv -> Predicted Label: 4\nFile: 35-MFCC.csv -> Predicted Label: 1\nFile: 49-MFCC.csv -> Predicted Label: 4\nFile: 47-MFCC.csv -> Predicted Label: 4\nFile: 14-MFCC.csv -> Predicted Label: 2\nFile: 26-MFCC.csv -> Predicted Label: 4\nFile: 100-MFCC.csv -> Predicted Label: 2\nFile: 89-MFCC.csv -> Predicted Label: 4\nFile: 96-MFCC.csv -> Predicted Label: 2\nFile: 18-MFCC.csv -> Predicted Label: 2\nFile: 105-MFCC.csv -> Predicted Label: 0\nFile: 81-MFCC.csv -> Predicted Label: 1\nFile: 56-MFCC.csv -> Predicted Label: 4\nFile: 58-MFCC.csv -> Predicted Label: 2\nFile: 39-MFCC.csv -> Predicted Label: 4\nFile: 54-MFCC.csv -> Predicted Label: 0\nFile: 62-MFCC.csv -> Predicted Label: 4\nFile: 93-MFCC.csv -> Predicted Label: 2\nFile: 78-MFCC.csv -> Predicted Label: 4\nFile: 101-MFCC.csv -> Predicted Label: 4\nFile: 21-MFCC.csv -> Predicted Label: 2\nFile: 109-MFCC.csv -> Predicted Label: 4\nFile: 40-MFCC.csv -> Predicted Label: 2\nFile: 116-MFCC.csv -> Predicted Label: 0\nFile: 31-MFCC.csv -> Predicted Label: 1\nFile: 84-MFCC.csv -> Predicted Label: 2\nFile: 28-MFCC.csv -> Predicted Label: 4\nFile: 19-MFCC.csv -> Predicted Label: 4\nFile: 102-MFCC.csv -> Predicted Label: 0\nFile: 22-MFCC.csv -> Predicted Label: 0\nFile: 34-MFCC.csv -> Predicted Label: 4\nFile: 57-MFCC.csv -> Predicted Label: 2\nFile: 75-MFCC.csv -> Predicted Label: 1\nFile: 24-MFCC.csv -> Predicted Label: 0\nFile: 30-MFCC.csv -> Predicted Label: 4\nFile: 43-MFCC.csv -> Predicted Label: 0\nFile: 115-MFCC.csv -> Predicted Label: 0\nFile: 71-MFCC.csv -> Predicted Label: 4\nFile: 82-MFCC.csv -> Predicted Label: 0\nFile: 07-MFCC.csv -> Predicted Label: 4\nFile: 38-MFCC.csv -> Predicted Label: 3\nFile: 52-MFCC.csv -> Predicted Label: 4\nFile: 42-MFCC.csv -> Predicted Label: 5\nFile: 60-MFCC.csv -> Predicted Label: 2\nFile: 99-MFCC.csv -> Predicted Label: 4\nFile: 41-MFCC.csv -> Predicted Label: 4\nFile: 85-MFCC.csv -> Predicted Label: 4\nFile: 103-MFCC.csv -> Predicted Label: 5\nFile: 80-MFCC.csv -> Predicted Label: 2\nFile: 114-MFCC.csv -> Predicted Label: 1\nFile: 37-MFCC.csv -> Predicted Label: 2\nFile: 97-MFCC.csv -> Predicted Label: 2\nFile: 05-MFCC.csv -> Predicted Label: 2\nFile: 12-MFCC.csv -> Predicted Label: 4\nFile: 08-MFCC.csv -> Predicted Label: 5\nFile: 04-MFCC.csv -> Predicted Label: 0\nFile: 51-MFCC.csv -> Predicted Label: 0\nFile: 13-MFCC.csv -> Predicted Label: 4\nFile: 17-MFCC.csv -> Predicted Label: 1\nFile: 11-MFCC.csv -> Predicted Label: 4\nFile: 59-MFCC.csv -> Predicted Label: 2\nFile: 69-MFCC.csv -> Predicted Label: 4\nFile: 88-MFCC.csv -> Predicted Label: 4\nFile: 113-MFCC.csv -> Predicted Label: 4\nFile: 66-MFCC.csv -> Predicted Label: 2\nFile: 68-MFCC.csv -> Predicted Label: 0\nFile: 45-MFCC.csv -> Predicted Label: 1\nFile: 95-MFCC.csv -> Predicted Label: 1\nFile: 50-MFCC.csv -> Predicted Label: 2\nFile: 02-MFCC.csv -> Predicted Label: 1\nFile: 27-MFCC.csv -> Predicted Label: 1\nFile: 106-MFCC.csv -> Predicted Label: 0\nFile: 87-MFCC.csv -> Predicted Label: 1\nFile: 76-MFCC.csv -> Predicted Label: 4\nFile: 20-MFCC.csv -> Predicted Label: 5\nFile: 108-MFCC.csv -> Predicted Label: 5\nFile: 55-MFCC.csv -> Predicted Label: 1\nFile: 98-MFCC.csv -> Predicted Label: 1\nFile: 15-MFCC.csv -> Predicted Label: 0\nFile: 90-MFCC.csv -> Predicted Label: 1\n","output_type":"stream"}],"execution_count":7}]}